<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>algorithm on Gra55&#39;s Blog</title>
    <link>/categories/algorithm/</link>
    <description>Recent content in algorithm on Gra55&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Tue, 14 Jan 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/algorithm/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>基础算法：分治法</title>
      <link>/blog/2020/algorithm/divide-and-conquer-algorithm/</link>
      <pubDate>Tue, 14 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/2020/algorithm/divide-and-conquer-algorithm/</guid>
      <description>0x00 算法概述 分治法的思想就是将不可能或者很难解决的问题，拆解成多个相似的子问题，然后将子问题拆解成更小粒度的子问题，直到这些小问题可以很容易的被解决，然后合并这些小问题的解以得到最终的解。
分治法是很多高效算法的基础，例如快排，归并排序等
0x01 解题步骤 满足以下所有特征才能使用分治法：
 该问题缩小到一定规模，就可以很容易的解决 具有最优子结构，即小规模问题和大规模问题是相同的问题（其实就是递归思想，可以使用递归来解决） 子问题的解可以合并成为大问题的解（这是关键特性，如果不满足，可以使用动态规划或贪心算法） 子问题之间是独立的（不独立的话也可以使用分治法，但是就是比较复杂，需要处理公共的子问题）  步骤：
 分解：将原问题分解为若干个规模较小，相互独立，与原问题形式相同的子问题； 解决：若子问题规模较小而容易被解决则直接解，否则递归地解各个子问题 合并：将各个子问题的解合并为原问题的解。  0x02 实现方式  递归   参考：
📌 五大常用算法之一：分治算法</description>
    </item>
    
    <item>
      <title>基础算法：动态规划</title>
      <link>/blog/2020/algorithm/dynamic-programming/</link>
      <pubDate>Tue, 14 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/2020/algorithm/dynamic-programming/</guid>
      <description>0x00 算法概述 动态规划的思想类似于分支法，也是将待处理的问题拆分成多个子问题，按顺序求解子问题，前一个子问题的解为后一个子问题的求解提供了有用的信息。
0x01 解题步骤 满足以下所有特征才能使用动态规划：
 最优子结构：问题的最优解所包含的子问题的解也是最优的（因为只有这样，状态转移到最后得出的解才是全局最优解） 无后效性：子问题的状态一旦确定，不会受到后面子问题的影响 有重叠子问题：子问题之间不是独立的，后面的子问题会多次用到前面子问题的解（这个不是必要条件，这个条件会使动态规划算法更具优势）  步骤：
 动态规划一般用来处理多阶段决策类问题，一般由初始状态开始，通过中间阶段的决策，最后得到结束的状态。这样会实现一个决策序列：初始状态 → │决策1│ → │决策2│ →…→ │决策n│ → 结束状态
  划分阶段：按照问题特征，把问题划分成多个阶段，阶段必须是有序的或者可排序的，否则无法求解 确定状态：一般情况下，各阶段状态就是各子问题的解，这样才能推出最终的解 状态转移方程：由之前一个或者多个状态得出当前状态的公式（这个一般是最难找的） 寻找终止条件：状态转移方程一般是递推式，需要找到终止条件来结束程序  0x02 实现方式 动态规划最难的就是找状态和状态转移方程，有时候状态找不对就很难找到状态转移方程。
代码实现起来也不是很复杂。
 参考：
📌 五大常用算法之二：动态规划算法</description>
    </item>
    
    <item>
      <title>基础算法：贪心算法</title>
      <link>/blog/2020/algorithm/greedy-algorithm/</link>
      <pubDate>Tue, 14 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/2020/algorithm/greedy-algorithm/</guid>
      <description>0x00 算法概述 贪心算法是在求解问题的时候，总是选择当前最优的解，不考虑全局最优解。
选择当前最优的解的方法称为贪心策略，贪心策略必须保证拆分的子问题必须是无后效性的（当前状态不会影响之前的状态）。
0x01 解题步骤  分析问题，抽象成数学模型 将问题拆分成多个子问题（子问题必须是无后效性的） 求解每一个子问题，得到子问题的局部最优解 合并子问题的最优解，得出全局解  贪心算法的核心就是找贪心策略，然后证明贪心策略中子问题的最优解一定会得到全局最优解。
0x02 实现方式 适用场景：
 单源最短路经问题 最小生成树问题 可任意分割的背包问题。如果不可以任意分割，就需要用动态规划求解。 某些情况下，即使贪心算法不能得到整体最优解，但其最终结果近似于最优解。  贪心算法的实现很简单，只要能找到贪心策略，代码很容易写出来。
 参考：
📌 五大常用算法之三：贪心算法
📌 常见算法及问题场景——贪心算法</description>
    </item>
    
    <item>
      <title>基础算法：回溯法</title>
      <link>/blog/2020/algorithm/back-tracking/</link>
      <pubDate>Mon, 13 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/2020/algorithm/back-tracking/</guid>
      <description>0x00 算法概述 回溯算法其实是一种枚举算法（穷举法），是一种暴力解法，时间复杂度比较高。
回溯法通过深度优先遍历的方法来尝试所有可能的解，当发现某一个分支无法满足求解条件，立马退回一步重新选择（回溯），尝试其他路径。如果只求一个解时，搜索到可用解后就停止搜索；如果求问题的所有解，就必须遍历所有解空间。
0x01 解题步骤  针对所给问题，确定解空间：需要确定问题的解空间是否存在一个（最优）解 确定搜索规则 以深度优先策略开始搜索解空间，搜索过程中通过剪枝函数避免无效搜索  什么是剪枝函数？
 剪枝函数是对无效解的过滤策略（明知道这条路径走下去不会得到解或最优解，所以就提前回溯，提高效率） 可行性剪枝：提前判断当前路径无法求出解，就可以提前回溯 最优化剪枝：声明一个变量存储当前最优解，如果可以提前判断当前路径无法满足最优解的条件，就提前回溯 剪枝函数特别难找，好的剪枝函数可以极大的降低算法的时间复杂度 回溯通常是通过反转动态规划的步骤来实现的  0x02 实现方式  非递归 递归   参考：
📌 五大常用算法之四：回溯法
📌 “通用解题法”之回溯中的“剪枝”</description>
    </item>
    
    <item>
      <title>AVL Tree 概述</title>
      <link>/blog/2020/algorithm/avl-tree/</link>
      <pubDate>Fri, 10 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/2020/algorithm/avl-tree/</guid>
      <description>0x00 前言 最早出现的是二叉树，随后人们发现二叉树可以用来二分查找，所以出现了二叉搜索树。
但是二叉搜索树在极端情况会退化成单链表的形式，所以出现了平衡二叉搜索树，即AVL 树（AVL 取自发明它的两个人的名字首字母，话说老外都爱这个干，以自己名字命名各种算法）
但是AVL 树也是有缺陷的，删除和插入效率很低（因为需要旋转多次），所以出现了红黑树，红黑树不是严格的平衡树，所以查找效率可能会低一点。
本文主要介绍AVL 树，后续出单独的文章介绍红黑树。
0x01 特性  必须是一颗二叉搜索树 每个节点的左子树和右子树的高度差的绝对值不能大于 1 查找、插入、删除的平均和最坏时间复杂度都是 O(logn)  0x02 术语 平衡因子（Balance Factor）
 二叉树节点的左子树高度减去右子树高度的值，称为该节点的平衡因子  最小不平衡子树
 距离插入节点最近的，且平衡因子绝对值大于 1 的节点为根的树，就是最小不平衡子树  0x03 实现 节点结构 下面的节点结构包含了节点的高度，也可以存储平衡因子和父节点。
class TreeNode(object): def __init__(self, value): self.value = value self.left = None self.right = None self.height = 0 AVL 类提供的函数 失衡调整 - 左单旋（在最小不平衡子树的右子树中插入右孩子时）
+---+ +---+ +---+ | 4 | | 4 | | 5 | +---+ +---+ +---+ | | | | +---+ +---+ +---+ +---+ | 5 | | 5 | | 4 | | 6 | +---+ +---+ +---+ +---+ | +---+ | 6 | +---+ def leftRotation(proot): &amp;#34;&amp;#34;&amp;#34;单左旋转操作:param proot: 最小失衡子树的根节点:rtype: TreeNode&amp;#34;&amp;#34;&amp;#34; # 左旋 tmpNode = proot.</description>
    </item>
    
    <item>
      <title>LRU 算法概述</title>
      <link>/blog/2020/algorithm/lru-cache/</link>
      <pubDate>Mon, 06 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/2020/algorithm/lru-cache/</guid>
      <description>0x00 算法概述 LRU（Least-recently-used）：最近最少被使用的某个东西。最早使用在内存中，表示最近最少被使用的内存将会被释放。
0x01 算法实现 如果想要这个算法的 get 和 set 时间复杂度都为 O(1)，我们这里需要使用两种数据结构 dict 和双向链表。
dict 获取元素的时间复杂度是 O(1)，set 元素时通过字典先找到已存在的元素，然后将这个元素挪到链表的尾部，时间复杂度也是 O(1)。结构如下所示：
+----------+ +----------+ +----------+ +----------+ | key-root | | key-A | | key-B | | key-C | +----------+ +----+-----+ +----+-----+ +----+-----+ | | | | | | | | v v v v +----+-----------------+----------------+------------------+----------+ | hash function | +----+-----------------+----------------+------------------+----------+ | | | | v v v v +----+-----+ +----+-----+ +---+------+ +----+-----+ +------&amp;gt;+ +-----&amp;gt;+ +------+ +-----&amp;gt;+ +--------+ | | root | | A | | B | | C | | | +----+ +&amp;lt;-----+ +------+ +&amp;lt;-----+ +&amp;lt;---+ | | | +----------+ +----------+ +----------+ +----------+ | | | | | | | +--------------------------------------------------------------------------+ | +---------------------------------------------------------------------------------+ 0x02 代码实现 在 Python 3 的内置模块 functools 中（Python 2 中没有），给出了 lru_cache 的实现，这应该就是用 Python 来实现 lru cache 的最佳实践了。</description>
    </item>
    
    <item>
      <title>Leetcode：#32 最长有效括号</title>
      <link>/blog/2019/algorithm/leetcode-32-longest-valid-parentheses/</link>
      <pubDate>Mon, 23 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/algorithm/leetcode-32-longest-valid-parentheses/</guid>
      <description>0x00 题目描述 #32
给定一个只包含 ( 和 ) 的字符串，找出最长的包含有效括号的子串的长度。
输入: &amp;#34;(()&amp;#34; 输出: 2 解释: 最长有效括号子串为 &amp;#34;()&amp;#34; 输入: &amp;#34;)()())&amp;#34; 输出: 4 解释: 最长有效括号子串为 &amp;#34;()()&amp;#34; 0x01 约束  注意字符串为空的情况  0x02 题解    一般看到 最长、最优 等求最优解的题目，首先应该想到动态规划。
  看到一个题，你首先应该能想到用什么现有的算法来解决，如果第一眼匹配不到现有的算法，那基本这个题你是解不出来的。因为不是专门研究算法的同学，能想到新的优秀的算法是很难的。
   算法 1：无头绪 之前遇到一个类似的题，没有任何头绪。暴力破解肯定是能想出来的，但是是蠢办法。
算法 2：动态规划 找状态和状态转移方程（类似于最长回文子串）：
 状态：  dp[i][j] 表示从 i-j 的子串是不是有效的括号 第一次就将状态找错了，DP 也可以是一维的三维的，不要形成思维定式。这里就是一维的状态：dp[i] 中 i 表示在 s 中，下标以 i 结束的子串，有效括号的个数   状态转移方程：  DP 最不好找的就是转移方程 这里分 3 种情况：  以 ( 结尾的肯定不是有效括号，直接忽略处理 以 ) 结尾的字符串中，如果 i-1 是 (，那么 dp[i] = dp[i-2] + 2（前两个字符索引处对应的最长括号有效个数 + 新增的 1 个有效括号） 以 ) 结尾的字符串中，如果 i-1 是 )，那么久追溯到 i-1 索引处对应的有效括号的最左端，即 i-dp[i-1]，然后查看 i-dp[i-1]-1是不是 (，如果是的话，dp[i] = dp[i-1]+2+dp[i-dp[i-1]-2]，其中 dp[i-dp[i-1]-2] 是前面的有效括号个数      def longestValidParentheses(s): sLen = len(s) if sLen &amp;lt; 2: return 0 dp = [0] * sLen for i in range(1, sLen): if s[i] == &amp;#39;)&amp;#39;: if s[i-1] == &amp;#39;(&amp;#39;: tmp = 0 if i-2 &amp;lt; 0 else dp[i-2] dp[i] = tmp + 2 else: if i-dp[i-1]-1 &amp;gt;=0 and s[i-dp[i-1]-1] == &amp;#39;(&amp;#39;: dp[i] = dp[i-1] + 2 + dp[i-dp[i-1]-2] return max(dp) PS：动态规划的时间复杂度其实也不小（O(n²)），只是避免了很多重复的计算。</description>
    </item>
    
    <item>
      <title>Leetcode：#10 正则表达式匹配</title>
      <link>/blog/2019/algorithm/leetcode-10-regular-expression-matching/</link>
      <pubDate>Sun, 22 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/algorithm/leetcode-10-regular-expression-matching/</guid>
      <description>0x00 题目描述 #10
给定一个字符串 s 和一个字符规则 p，实现一个支持 . 和 * 的正则表达式匹配。其中 . 匹配任意单个字符，* 匹配零个或多个前面的那一个元素。
所谓匹配，是要涵盖整个字符串 s，而不是 s 的子串。类似于 re.match 而不是 re.search。
输入: s = &amp;#34;aa&amp;#34; p = &amp;#34;a&amp;#34; 输出: false 解释: &amp;#34;a&amp;#34; 无法匹配 &amp;#34;aa&amp;#34; 整个字符串。 0x01 约束  s 可能为空，且只包含 a-z 的小写字母 p 可能为空，且只包含 a-z 的小写字母，以及字符 . 和 *  0x02 题解    一般看到 最长、最优 等求最优解的题目，首先应该想到动态规划。
  看到一个题，你首先应该能想到用什么现有的算法来解决，如果第一眼匹配不到现有的算法，那基本这个题你是解不出来的。因为不是专门研究算法的同学，能想到新的优秀的算法是很难的。
   算法 1：无头绪 开始拿到这个题，我感觉无从下手，想不到什么好的办法（没有头绪）。
只是想到 s 和 p 各拿一个指针，从左到右的扫描。接下来需要进行各种 if else 的判断，而且判断条件很复杂，不一定能做出来。</description>
    </item>
    
    <item>
      <title>Leetcode：#204 计数质数</title>
      <link>/blog/2019/algorithm/leetcode-204-count-primes/</link>
      <pubDate>Sun, 22 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/algorithm/leetcode-204-count-primes/</guid>
      <description>0x00 题目描述 #204
统计所有小于非负整数 n 的质数的数量。
0x01 约束   n 为非负整数
  0 和 1 不是质数
  n 溢出的情况
  0x02 题解 什么是质数？ 质数就是只能被 1 和 自己整除的数字，也叫素数。
算法 1：暴力破解法 暴力破解法是最简单也是最容易想到的算法
 遍历 n 个数字，判断每个数字是不是质数，判断是不是质数的方法是遍历小于它的每个数字，看能不能被整除。  这个算法太愚蠢了，时间复杂度也很大 O(n²)。
算法 2：埃拉托斯特尼筛法 我最初卡在如何判断一个数字是不是质数的问题上。其实判断一个数是不是质数，不需要判断比他小的每一个数字可不可以被整除，只需要判断到 [1-sqrt(n)]。
比如说 12 = 2 x 6，12 = 3 x 4，12 = sqrt(12) x sqrt(12)，12 = 4 x 3，12 = 6 x 2。从 sqrt(12) 分割，前半部分和后半部分是对称的，所以只需要判断 [1-sqrt(n)] 区间内的数字可不可以被整除就 ok 了。</description>
    </item>
    
    <item>
      <title>Leetcode：#5 最长回文子串</title>
      <link>/blog/2019/algorithm/leetcode-5-longest-palindromic-substring/</link>
      <pubDate>Sun, 22 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/algorithm/leetcode-5-longest-palindromic-substring/</guid>
      <description>0x00 题目描述 #5
给定一个字符串 s，找到 s 中最长的回文子串。
输入: &amp;#34;babad&amp;#34; 输出: &amp;#34;bab&amp;#34; 注意: &amp;#34;aba&amp;#34; 也是一个有效答案。 0x01 约束  s 最大长度为 1000 注意字符串为空的情况  0x02 题解    一般看到 最长、最优 等求最优解的题目，首先应该想到动态规划。
  看到一个题，你首先应该能想到用什么现有的算法来解决，如果第一眼匹配不到现有的算法，那基本这个题你是解不出来的。因为不是专门研究算法的同学，能想到新的优秀的算法是很难的。
   算法 1：暴力破解法 这是一个蠢方法，把所有的子串找出来，判断每个子串是不是回文的，然后找出最长的那个子串返回。
找出所有子串的时间复杂度是 O(n²)。
算法 2：动态规划 找状态和状态转移方程：
 状态：dp[i][j] 表示从 i-j 的子串是不是回文的 状态转移方程：dp[i][j] = s[i] == s[j] and (j-1 &amp;lt;= 2 or dp[i+1][j-1] == True)  def longestPalindrome(s): sLen = len(s) if sLen == 0: return s dp = [[0] * sLen for _ in range(sLen)] ret = s[0] for j in range(1, sLen): # 如果使用 range(sLen) 来循环，提交会超时，可能与题目限制 s 最大为 1000 有关，多一次循环就给超时了 for i in range(j): if s[i] == s[j] and (j-i &amp;lt;= 2 or dp[i+1][j-1]): dp[i][j] = 1 if j-i+1 &amp;gt; len(ret): ret = s[i:j+1] return ret PS：动态规划的时间复杂度其实也不小（O(n²)），只是避免了很多重复的计算。</description>
    </item>
    
  </channel>
</rss>